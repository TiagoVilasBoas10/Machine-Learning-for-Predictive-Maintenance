{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción de anomalías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "#from sklearn.model_selection import StratifiedKFold\n",
    "#from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y preparación del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./Anomaly Prediction/cls_balanced_extended_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algunos valores calculados de 'Roll_Std' son nan\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Eliminación de features con valores fuera del rango [0, 1]\n",
    "features = ['Day', 'Month', 'Year', 'DeviceID']\n",
    "df[[feature for feature in df.columns if not feature in features]]\n",
    "\n",
    "# Eliminación de features con desviación estándar = 0, nada útiles para modelar\n",
    "no_std, low_std = {}, {}\n",
    "for column in df.columns:\n",
    "    try:\n",
    "        if df[column].std() == 0:\n",
    "            no_std[column] = df[column].std()\n",
    "        \n",
    "        if df[column].std() < 0.01:\n",
    "            low_std[column] = df[column].std()\n",
    "    except:\n",
    "        None\n",
    "\n",
    "# Dos datasets: uno con todas las features, otro con sólo aquellas features cuya desviación estándar es mayor a 0.01\n",
    "df_all = df.drop([k for k in no_std.keys()], axis=1)\n",
    "df_filtered = df.drop([k for k in low_std.keys()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117440, 997)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.to_pickle('./Anomaly Prediction/dataset_ready_MLP_1.pkl')\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117440, 338)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.to_pickle('./Anomaly Prediction/dataset_ready_MLP_2.pkl')\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación mediante MLP: MultiLayer Perceptron (todas las features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ayfdcp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ayfdcp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\ayfdcp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 83676 samples, validate on 4404 samples\n",
      "Epoch 1/15\n",
      "83676/83676 [==============================] - 30s 355us/step - loss: 0.3595 - acc: 0.8814 - val_loss: 1.6433 - val_acc: 0.8980\n",
      "Epoch 2/15\n",
      "83676/83676 [==============================] - 31s 368us/step - loss: 0.3275 - acc: 0.8996 - val_loss: 1.6433 - val_acc: 0.8980\n",
      "Epoch 3/15\n",
      "83676/83676 [==============================] - 30s 353us/step - loss: 0.3257 - acc: 0.9000 - val_loss: 0.3114 - val_acc: 0.8999\n",
      "Epoch 4/15\n",
      "83676/83676 [==============================] - 29s 341us/step - loss: 0.3240 - acc: 0.9005 - val_loss: 1.1466 - val_acc: 0.8980\n",
      "Epoch 5/15\n",
      "83676/83676 [==============================] - 30s 363us/step - loss: 0.3234 - acc: 0.9010 - val_loss: 0.4528 - val_acc: 0.8980\n",
      "Epoch 6/15\n",
      "83676/83676 [==============================] - 32s 388us/step - loss: 0.3218 - acc: 0.9011 - val_loss: 1.5236 - val_acc: 0.8980\n",
      "Epoch 7/15\n",
      "83676/83676 [==============================] - 29s 346us/step - loss: 0.3233 - acc: 0.9012 - val_loss: 0.5410 - val_acc: 0.7078\n",
      "Epoch 8/15\n",
      "83676/83676 [==============================] - 31s 365us/step - loss: 0.3205 - acc: 0.9008 - val_loss: 0.6916 - val_acc: 0.8980\n",
      "Epoch 9/15\n",
      "83676/83676 [==============================] - 29s 351us/step - loss: 0.3202 - acc: 0.9011 - val_loss: 3.9319 - val_acc: 0.1020\n",
      "Epoch 10/15\n",
      "83676/83676 [==============================] - 32s 387us/step - loss: 0.3207 - acc: 0.9015 - val_loss: 0.7323 - val_acc: 0.8980\n",
      "Epoch 11/15\n",
      "83676/83676 [==============================] - 34s 411us/step - loss: 0.3175 - acc: 0.9028 - val_loss: 1.4401 - val_acc: 0.8980\n",
      "Epoch 12/15\n",
      "83676/83676 [==============================] - 31s 366us/step - loss: 0.3194 - acc: 0.9014 - val_loss: 7.4181 - val_acc: 0.1020\n",
      "Epoch 13/15\n",
      "83676/83676 [==============================] - 32s 378us/step - loss: 0.3178 - acc: 0.9021 - val_loss: 1.0080 - val_acc: 0.8980\n",
      "Epoch 14/15\n",
      "83676/83676 [==============================] - 31s 367us/step - loss: 0.3172 - acc: 0.9018 - val_loss: 1.2829 - val_acc: 0.8980\n",
      "Epoch 15/15\n",
      "83676/83676 [==============================] - 31s 374us/step - loss: 0.3174 - acc: 0.9020 - val_loss: 0.8719 - val_acc: 0.8980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21881eda198>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_all[[feature for feature in df_all.columns if not feature == 'Problem']].values\n",
    "y = df_all['Problem'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2019, shuffle=True, stratify=y)\n",
    "\n",
    "seed = 2019\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(X_train.shape[1], activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=15, batch_size=128, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo\n",
    "pickle.dump(model, open('./Anomaly Prediction/MLP/MLP_model_1.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación mediante MLP: MultiLayer Perceptron (features filtradas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 83676 samples, validate on 4404 samples\n",
      "Epoch 1/15\n",
      "83676/83676 [==============================] - 6s 66us/step - loss: 0.3397 - acc: 0.8877 - val_loss: 4.2958 - val_acc: 0.1020\n",
      "Epoch 2/15\n",
      "83676/83676 [==============================] - 5s 61us/step - loss: 0.3078 - acc: 0.9029 - val_loss: 1.6293 - val_acc: 0.8980\n",
      "Epoch 3/15\n",
      "83676/83676 [==============================] - 5s 64us/step - loss: 0.3061 - acc: 0.9039 - val_loss: 0.3436 - val_acc: 0.8983\n",
      "Epoch 4/15\n",
      "83676/83676 [==============================] - 5s 55us/step - loss: 0.3050 - acc: 0.9038 - val_loss: 1.9042 - val_acc: 0.1020\n",
      "Epoch 5/15\n",
      "83676/83676 [==============================] - 5s 55us/step - loss: 0.3043 - acc: 0.9041 - val_loss: 0.8694 - val_acc: 0.8980\n",
      "Epoch 6/15\n",
      "83676/83676 [==============================] - 5s 59us/step - loss: 0.3041 - acc: 0.9042 - val_loss: 1.2762 - val_acc: 0.8980\n",
      "Epoch 7/15\n",
      "83676/83676 [==============================] - 5s 56us/step - loss: 0.3027 - acc: 0.9043 - val_loss: 0.5109 - val_acc: 0.7498\n",
      "Epoch 8/15\n",
      "83676/83676 [==============================] - 5s 59us/step - loss: 0.3030 - acc: 0.9048 - val_loss: 1.6433 - val_acc: 0.8980\n",
      "Epoch 9/15\n",
      "83676/83676 [==============================] - 6s 66us/step - loss: 0.3026 - acc: 0.9044 - val_loss: 0.3280 - val_acc: 0.9024\n",
      "Epoch 10/15\n",
      "83676/83676 [==============================] - 5s 61us/step - loss: 0.3028 - acc: 0.9040 - val_loss: 0.4556 - val_acc: 0.8980\n",
      "Epoch 11/15\n",
      "83676/83676 [==============================] - 5s 62us/step - loss: 0.3017 - acc: 0.9047 - val_loss: 2.2947 - val_acc: 0.1020\n",
      "Epoch 12/15\n",
      "83676/83676 [==============================] - 5s 64us/step - loss: 0.3023 - acc: 0.9038 - val_loss: 10.3071 - val_acc: 0.1020\n",
      "Epoch 13/15\n",
      "83676/83676 [==============================] - 5s 63us/step - loss: 0.3019 - acc: 0.9049 - val_loss: 0.7949 - val_acc: 0.8980\n",
      "Epoch 14/15\n",
      "83676/83676 [==============================] - 5s 64us/step - loss: 0.3018 - acc: 0.9046 - val_loss: 0.8425 - val_acc: 0.8980\n",
      "Epoch 15/15\n",
      "83676/83676 [==============================] - 6s 71us/step - loss: 0.3020 - acc: 0.9042 - val_loss: 0.8367 - val_acc: 0.8980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x218891bc668>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_filtered[[feature for feature in df_filtered.columns if not feature == 'Problem']].values\n",
    "y = df_filtered['Problem'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2019, shuffle=True, stratify=y)\n",
    "\n",
    "seed = 2019\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(X_train.shape[1], activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=15, batch_size=128, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo\n",
    "pickle.dump(model, open('./Anomaly Prediction/MLP/MLP_model_2.sav', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
