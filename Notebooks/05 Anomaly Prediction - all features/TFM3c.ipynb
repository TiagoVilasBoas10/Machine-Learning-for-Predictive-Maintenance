{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción de anomalías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración del dataset: balance adecuado entre las dos clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('sampledata2.pkl')\n",
    "\n",
    "# Feature 'Problem': por cada día con 'ProblemReported' = 1, etiquetamos dicho día y los 2 días anteriores.\n",
    "for device in df.DeviceID.unique():\n",
    "    reported_array = df.loc[(df.DeviceID == device), 'ProblemReported'].values\n",
    "    reported_array = reported_array[::-1]             # Recorreremos el vector al contrario, facilitando la implementación\n",
    "    problem_array = np.zeros(len(reported_array))\n",
    "    \n",
    "    for i in range(len(reported_array)):\n",
    "        if reported_array[i] == 1:\n",
    "            \n",
    "            for j in range(i, i + 3):             # En adelante al problema reportado, etiquetamos 2 días adicionales\n",
    "                if j < len(reported_array):           # Evitamos el acceso a un índice fuera de los límites del array\n",
    "                    problem_array[j] = 1\n",
    "    \n",
    "    problem_array = problem_array[::-1]\n",
    "    df.loc[df.DeviceID == device, 'Problem'] = problem_array\n",
    "\n",
    "df_problem_reported = df.loc[(df.Problem == 1)]\n",
    "df_no_problem_reported = df.loc[(df.Problem == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117440, 135)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selección de features\n",
    "features = ['Day', 'Month', 'Year', 'Problem']\n",
    "features.extend([feature for feature in df.columns if 'Error' in feature and len(feature) >= 13])\n",
    "features.extend([feature for feature in df.columns if 'Problem' in feature and len(feature) >= 26])\n",
    "features.extend([feature for feature in df.columns if 'Fault' in feature and len(feature) >= 29])\n",
    "features.extend([feature for feature in df.columns if 'Warning_Type' in feature and len(feature) >= 26])\n",
    "features.extend([feature for feature in df.columns if 'AE' in feature])\n",
    "features.extend([feature for feature in df.columns if 'Grouped' in feature])\n",
    "\n",
    "# Reducción de registros para mantener un balance 90-10 (10% de anomalías, 90% sin anomalías) entre clases\n",
    "np.random.seed(2019)\n",
    "remove_n = df_no_problem_reported.shape[0] - (df_problem_reported.shape[0] * 9) # Cálculo del número de filas a borrar\n",
    "drop_indices = np.random.choice(df_no_problem_reported.index, remove_n, replace=False)\n",
    "df_no_problem_reported = df_no_problem_reported.drop(drop_indices)\n",
    "\n",
    "# Eliminación de features innecesarias en los 2 dataframes base y creación del dataframe final\n",
    "df_problem_reported.drop(columns=[feature for feature in df_problem_reported.columns if not feature in features], inplace=True)\n",
    "df_no_problem_reported.drop(columns=[feature for feature in df_no_problem_reported.columns if not feature in features], inplace=True)\n",
    "df_B = pd.concat([df_no_problem_reported, df_problem_reported], ignore_index=True)\n",
    "\n",
    "df_B.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos de los 5 ficheros 'df_rolling_*d.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117440, 1095)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_windows = ['3d', '7d', '15d', '30d', '90d']\n",
    "rolling_files = ['./df_rolling_' + time_w + '.pkl' for time_w in time_windows]\n",
    "\n",
    "for r_file in rolling_files:\n",
    "    # Carga del fichero y retención de las features móviles (rolling)\n",
    "    df_rolling = pd.read_pickle(r_file)\n",
    "    df_rolling.drop(columns=[feature for feature in df_rolling.columns if not 'Roll' in feature], inplace=True)\n",
    "    \n",
    "    # Unión de dataframes por separado para mantener los índices anteriormente seleccionados\n",
    "    df_npr_temp = pd.merge(df_no_problem_reported, df_rolling, how='inner', left_index=True, right_index=True)\n",
    "    df_pr_temp = pd.merge(df_problem_reported, df_rolling, how='inner', left_index=True, right_index=True)\n",
    "    \n",
    "    # Retención de las features móviles (nueva operación 'drop', necesaria para evitar el mensaje de 'Memory Error')\n",
    "    df_npr_temp.drop(columns=[feature for feature in df_npr_temp.columns if not 'Roll' in feature], inplace=True)\n",
    "    df_pr_temp.drop(columns=[feature for feature in df_pr_temp.columns if not 'Roll' in feature], inplace=True)\n",
    "    \n",
    "    # Unión de los 2 dataframes creados en esta iteración y concatenación con el dataframe final\n",
    "    df_rolling = pd.concat([df_npr_temp, df_pr_temp], ignore_index=True)\n",
    "    df_B = pd.concat([df_B, df_rolling], axis=1)\n",
    "\n",
    "df_B.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardado del dataset extendido (features categóricas y numéricas) para predicción de anomalías mediante clasificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "try:\n",
    "    os.mkdir('Anomaly Prediction')\n",
    "except:\n",
    "    None\n",
    "\n",
    "df_B.to_pickle('./Anomaly Prediction/cls_balanced_extended_dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargado del dataset extendido y ejecución de una batería de modelos de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n",
      "Imputing missing values in feature set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=60, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped pipeline #1 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #3 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #6 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #9 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #11 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #15 due to time out. Continuing to the next pipeline.\n",
      "Saving periodic pipeline from pareto front to tpot_models\\pipeline_gen_1_idx_0_2019.05.19_11-42-17.py\n",
      "Skipped pipeline #18 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #21 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #23 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #25 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #27 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #29 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #32 due to time out. Continuing to the next pipeline.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.9072093551316985\tLogisticRegression(input_matrix, LogisticRegression__C=0.5, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Skipped pipeline #36 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #38 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #41 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #43 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #46 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #48 due to time out. Continuing to the next pipeline.\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.9072093551316985\tLogisticRegression(input_matrix, LogisticRegression__C=0.5, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Skipped pipeline #54 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #56 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #58 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #61 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #63 due to time out. Continuing to the next pipeline.\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.9072093551316985\tLogisticRegression(input_matrix, LogisticRegression__C=0.5, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Skipped pipeline #66 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #69 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #73 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #75 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #77 due to time out. Continuing to the next pipeline.\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.9072093551316985\tLogisticRegression(input_matrix, LogisticRegression__C=0.5, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "Skipped pipeline #85 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #89 due to time out. Continuing to the next pipeline.\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.9072093551316985\tLogisticRegression(input_matrix, LogisticRegression__C=0.5, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Imputing missing values in feature set\n",
      "0.9061648501362398\n",
      "Imputing missing values in feature set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.91      0.99      0.95     26424\n",
      "        1.0       0.67      0.12      0.21      2936\n",
      "\n",
      "avg / total       0.89      0.91      0.88     29360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_B = pd.read_pickle('./Anomaly Prediction/cls_balanced_extended_dataset.pkl')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "X = df_B[[feature for feature in df_B.columns if not feature == 'Problem']].values\n",
    "y = df_B['Problem'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2019, shuffle=True, stratify=y)\n",
    "tpot = TPOTClassifier(verbosity=3, scoring='accuracy', random_state=2019, periodic_checkpoint_folder='tpot_models', \n",
    "                      n_jobs=-1, generations=5, population_size=10, cv=3)\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "\n",
    "tpot.export('./Anomaly Prediction/best_extended_model_code.py')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = tpot.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
