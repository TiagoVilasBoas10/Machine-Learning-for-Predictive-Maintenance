{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción de anomalías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "from keras.layers.core import Dense \n",
    "from keras.models import Model, Sequential\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balance adecuado entre las dos clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('sampledata2.pkl')\n",
    "\n",
    "# Feature 'Problem': por cada día con 'ProblemReported' = 1, etiquetamos dicho día y los 2 días anteriores.\n",
    "for device in df.DeviceID.unique():\n",
    "    reported_array = df.loc[(df.DeviceID == device), 'ProblemReported'].values\n",
    "    reported_array = reported_array[::-1]             # Recorreremos el vector al contrario, facilitando la implementación\n",
    "    problem_array = np.zeros(len(reported_array))\n",
    "    \n",
    "    for i in range(len(reported_array)):\n",
    "        if reported_array[i] == 1:\n",
    "            \n",
    "            for j in range(i, i + 3):             # En adelante al problema reportado, etiquetamos 2 días adicionales\n",
    "                if j < len(reported_array):           # Evitamos el acceso a un índice fuera de los límites del array\n",
    "                    problem_array[j] = 1\n",
    "    \n",
    "    problem_array = problem_array[::-1]\n",
    "    df.loc[df.DeviceID == device, 'Problem'] = problem_array\n",
    "\n",
    "df_problem_reported = df.loc[(df.Problem == 1)]\n",
    "df_no_problem_reported = df.loc[(df.Problem == 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selección de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Day', 'Month', 'Year', 'Problem']\n",
    "features.extend([feature for feature in df.columns if 'Error' in feature and len(feature) == 15])\n",
    "features.extend([feature for feature in df.columns if 'Grouped' in feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) Dataset para predicción de anomalías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117440, 87)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reducción de registros para mantener un balance 90-10 (10% de anomalías, 90% sin anomalías) entre clases\n",
    "np.random.seed(2019)\n",
    "remove_n = df_no_problem_reported.shape[0] - (df_problem_reported.shape[0] * 9) # Cálculo del número de filas a borrar\n",
    "drop_indices = np.random.choice(df_no_problem_reported.index, remove_n, replace=False)\n",
    "df_no_problem_reported = df_no_problem_reported.drop(drop_indices)\n",
    "\n",
    "df_B = pd.concat([df_no_problem_reported, df_problem_reported], ignore_index=True)\n",
    "df_B.drop(columns=[feature for feature in df_B.columns if not feature in features], inplace=True)\n",
    "\n",
    "df_B.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardado del dataset para predicción de anomalías mediante modelos de clasificación sobre variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "try:\n",
    "    os.mkdir('Anomaly Prediction')\n",
    "except:\n",
    "    None\n",
    "\n",
    "df_B.to_pickle('./Anomaly Prediction/cls_balanced_dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecución de una batería de modelos de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_B = pd.read_pickle('./Anomaly Prediction/cls_balanced_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=60, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving periodic pipeline from pareto front to tpot_models\\pipeline_gen_1_idx_0_2019.05.19_01-46-50.py\n",
      "Skipped pipeline #12 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #17 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #20 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #23 due to time out. Continuing to the next pipeline.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.907050408719346\tLogisticRegression(input_matrix, LogisticRegression__C=0.5, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Skipped pipeline #25 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #30 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #35 due to time out. Continuing to the next pipeline.\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.907050408719346\tLogisticRegression(input_matrix, LogisticRegression__C=0.5, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "-2\t0.9071185286103542\tLogisticRegression(SelectPercentile(input_matrix, SelectPercentile__percentile=21), LogisticRegression__C=0.5, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Saving periodic pipeline from pareto front to tpot_models\\pipeline_gen_2_idx_1_2019.05.19_01-57-36.py\n",
      "Skipped pipeline #47 due to time out. Continuing to the next pipeline.\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.9078792007266122\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=4, GradientBoostingClassifier__max_features=0.25, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=18, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "Saving periodic pipeline from pareto front to tpot_models\\pipeline_gen_3_idx_0_2019.05.19_02-04-26.py\n",
      "Skipped pipeline #49 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #52 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #55 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #57 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #59 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #61 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #63 due to time out. Continuing to the next pipeline.\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.9078792007266122\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=4, GradientBoostingClassifier__max_features=0.25, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=18, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Skipped pipeline #67 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #69 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #71 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #73 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #75 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #77 due to time out. Continuing to the next pipeline.\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.9078792007266122\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=4, GradientBoostingClassifier__max_features=0.25, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=18, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "0.9066757493188011\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "X = df_B.iloc[:, :-1].values\n",
    "y = df_B.iloc[:, -1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2019, shuffle=True, stratify=y)\n",
    "tpot = TPOTClassifier(verbosity=3, scoring='accuracy', random_state=2019, periodic_checkpoint_folder='tpot_models', \n",
    "                      n_jobs=-1, generations=5, population_size=10, cv=3)\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "\n",
    "tpot.export('./Anomaly Prediction/best_model_code.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicción del conjunto de test y obtención de indicadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.91      0.99      0.95     26424\n",
      "        1.0       0.66      0.14      0.23      2936\n",
      "\n",
      "avg / total       0.89      0.91      0.88     29360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = tpot.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
